{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73d16049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import MultipleLocator\n",
    "import binascii\n",
    "\n",
    "globalShingleList = []\n",
    "globalShingleRDD = [0]*101 # 要變成 RDD\n",
    "globalShingleLen = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a8de6",
   "metadata": {},
   "source": [
    "### 計算word weight\n",
    "\n",
    "因為檔案中包含的文字包括數字和英文  \n",
    "為了確保取出的word有意義，因此設計一個計算word weight的方式  \n",
    "在weight高於一定數值的時候才把他當成有意義的字  \n",
    "並建立(1, [word])的tuple，加入wordList中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15a35469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_filter_word(line):\n",
    "    words = line.split(\" \")\n",
    "    wordList = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word != \"\":\n",
    "            weight = 0\n",
    "            for c in word:\n",
    "                if (ord(c) >= 65 and ord(c) <= 90) or (ord(c) >= 97 and ord(c) <= 122): # 65~90小寫字母, 97~122大寫字母\n",
    "                    weight += 1\n",
    "                elif ord(c) >= 48 and ord(c) <= 57: # 48~57數字\n",
    "                    weight += 0.5\n",
    "                else:\n",
    "                    weight += 0.1\n",
    "            \n",
    "            if weight > (len(word)/3)*2:\n",
    "                wordList.append((1, [word]))\n",
    "                \n",
    "    return wordList "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd10dea",
   "metadata": {},
   "source": [
    "### 建立Shingle  \n",
    "\n",
    "輸入line = (1, [words])，[words]是包含所有word的list  \n",
    "每三個字的字串建立一個shingle, 用binascii.crc32計算crc32加密結果作為shingle value  \n",
    "並存成shingleList=[shingle1, shingle2...]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eeae281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_encode(line):\n",
    "    shingleList = []\n",
    "    length = len(line[1])\n",
    "    \n",
    "    i = 0\n",
    "    for word in line[1]:\n",
    "        if i == length-2: break\n",
    "        else:\n",
    "            shingle = line[1][i] + line[1][i+1] + line[1][i+2]\n",
    "            shingle = shingle.encode()\n",
    "            shingleNum = binascii.crc32(shingle)\n",
    "            shingleList.append(shingleNum)\n",
    "        i += 1\n",
    "        \n",
    "    return shingleList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45982f5b",
   "metadata": {},
   "source": [
    "### shingle value轉換成對應shingle id\n",
    "對輸入的line=[docID, shingle1, shingle2, ...]  \n",
    "尋找該shingle在globalShingleList中對應到的index  \n",
    "存成(docID, [shingleIndex])形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f604292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_setUpShingleId(line):\n",
    "    shingleList = []\n",
    "    length = len(line)\n",
    "    docId = line[0]\n",
    "    \n",
    "    for i in range(1, length):\n",
    "        shingleIndex = globalShingleList.index(line[i])\n",
    "        shingleList.append((docId, [shingleIndex]))\n",
    "        \n",
    "    return shingleList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1974aad7",
   "metadata": {},
   "source": [
    "### minhash 計算hash\n",
    "line = (docID, [shingleList])  \n",
    "對shingleList中所有shingleID計算hash的結果\n",
    "需要使用100種hash function進行計算  \n",
    "使用hash function產生方式為 j*(j+1)+shingle*(j+2)+j % globalShingleLen  \n",
    "並對持續更新100個hashVal的值  \n",
    "最終產生minhashList = (docID, [hash1, ..., hash100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfae789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_minhash(line):\n",
    "    minHashList = []\n",
    "    length = len(line[1])\n",
    "    docId = line[0]\n",
    "    \n",
    "    hashVal = [globalShingleLen]*100\n",
    "     \n",
    "    for i in range(length):\n",
    "        for j in range(100):\n",
    "            val = (j*(j+1) + line[1][i]*(j+2) + j) % globalShingleLen\n",
    "            if val < hashVal[j]: \n",
    "                hashVal[j] = val\n",
    "                \n",
    "    minHashList.append((docId, hashVal))\n",
    "    \n",
    "    return minHashList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdcb06d",
   "metadata": {},
   "source": [
    "### LSH hash\n",
    "line = (0, [hash1~hash100])  \n",
    "因為 r=2, b=50  \n",
    "對每兩個item進行division hash, hash function = (item1*item2)%10007，共計算50次  \n",
    "算出來的結果與對應的document ID建立成一個((bandID, hash_value)， [docID])的tuple, 存進LSHList中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52648527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_LSH(line):\n",
    "    LSHlist = []\n",
    "    docId = line[0]\n",
    "    \n",
    "    for i in range(50):\n",
    "        item1 = line[1][2*i]\n",
    "        item2 = line[1][2*i + 1]\n",
    "        bucket = (item1*item2)%10007\n",
    "        \n",
    "        LSHlist.append(((i+1, bucket), [docId]))\n",
    "        \n",
    "    return LSHlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577ee2d1",
   "metadata": {},
   "source": [
    "### 計算candidate pair\n",
    "line = ((bandID, hash_value), [doc1, doc2, ...])  \n",
    "檢查hash到一樣value的document數量，兩個以上的，其中所有doc為candidate pair  \n",
    "並回傳((candidate1, candidate2, ...), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6d3f7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_filter_candidate(line):\n",
    "    candidateList = []\n",
    "    candidate_length = len(line[1])\n",
    "    candidates = line[1]\n",
    "    \n",
    "    if candidate_length <= 1: pass\n",
    "    else:\n",
    "        candidateList.append((tuple(candidates), 1))\n",
    "    return candidateList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1baa5ad",
   "metadata": {},
   "source": [
    "### 計算similarity\n",
    "line = (candidate list, 同一組candidate list出現的頻率)  \n",
    "對candidateList中所有candidate pair  \n",
    "透過先前算好的sigMatrix（包含所有document的100個hash value）  \n",
    "計算similarity，也就是hash value相同的機率  \n",
    "並將結果((candidate1, canditate2), similarity)存進simList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "309571b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_calSim(line):\n",
    "    simList = []\n",
    "    length = len(line[0])\n",
    "    \n",
    "    for i in range(length): # 有多少 candidate\n",
    "        for j in range(i+1, length):\n",
    "            match = 0\n",
    "            for h in range(100):\n",
    "                if sigMatrix[line[0][i]-1][h] == sigMatrix[line[0][j]-1][h]:\n",
    "                    match += 1\n",
    "                    \n",
    "            sim = match / 100\n",
    "            simList.append(((line[0][i], line[0][j]), sim))\n",
    "            \n",
    "    return simList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00bd0651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_switch(line):\n",
    "    return (line[1], line[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a58328",
   "metadata": {},
   "source": [
    "### Step1 : Shingle\n",
    "\n",
    "讀進所有document  \n",
    "對每一個document，把document中的單詞建立成[(1, [word1, word2, ...]]的rdd  \n",
    "將所有document rdd建立成[shingle1, .shingle2,...]的shingles rdd,  \n",
    "再將shingles rdd轉成shingle list  \n",
    "\n",
    "接著檢查所有不重複的shingle，暫存在localShingleList中，進行排序後存進globalShingleRDD  \n",
    "globalShingleRDD是準備轉成rdd的2d array\n",
    "對每個document i，我們將document的localShingleListh存成[docID, shingle1, shingle2...]的格式\n",
    "並存入globalShingleRDD中  \n",
    "並且另外建立globalShingleList，存入所有document的shingles  \n",
    "\n",
    "建立好所有需要的list後，把globalShingleRDD轉成rdd  \n",
    "並且用mapper_setUpShingleId把其中所有shingle value轉換成shingle index形式\n",
    "最終產生一個[(0, [shingleList]), (1, [shingleList]), ....] 格式的doc_shingleId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "144777ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/licairong/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/Users/licairong/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"TermProject\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "for i in range(101):\n",
    "    if i >=0 and i <=8: fileName = \"./athletics/\" + \"00\" + str(i+1) + \".txt\"\n",
    "    elif i >= 9 and i <= 98: fileName = \"./athletics/\" + \"0\" + str(i+1) + \".txt\"\n",
    "    else: fileName = \"./athletics/\" + str(i+1) + \".txt\"\n",
    "        \n",
    "    document = sc.textFile(fileName).flatMap(mapper_filter_word).reduceByKey(lambda x, y: x+y)\n",
    "    \n",
    "    # encode shingle\n",
    "    localShingleList = []\n",
    "    localShingleList.append(i) # 第幾個 document\n",
    "    \n",
    "    shingles = document.flatMap(mapper_encode)\n",
    "    shingleNum = shingles.count() # # of shingles (per document)\n",
    "    shingleList = shingles.take(shingleNum) # 從 RDD 變成 list\n",
    "    \n",
    "    for j in range(shingleNum):\n",
    "        if shingleList[j] not in localShingleList:\n",
    "            localShingleList.append(shingleList[j])\n",
    "        if shingleList[j] not in globalShingleList:\n",
    "            globalShingleList.append(shingleList[j])\n",
    "    localShingleList.sort()\n",
    "    globalShingleRDD[i-1] = localShingleList\n",
    "    \n",
    "globalShingleList.sort()\n",
    "globalShingleLen = len(globalShingleList)\n",
    "\n",
    "doc_shingleId = sc.parallelize(globalShingleRDD).flatMap(mapper_setUpShingleId).reduceByKey(lambda x, y: x+y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552c0c71",
   "metadata": {},
   "source": [
    "### Step2 : Min hashing\n",
    "\n",
    "對doc_shingleId內存的所有single用mapper_minhash進行hash\n",
    "產生minhashResult = [(0, [hash1, ..., hash100]), (1, [hash1, ..., hash100]),...]  \n",
    "轉換成signature matrix=[  \n",
    "hashList0,  \n",
    "hashList1,  \n",
    "...  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1e7fb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/licairong/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/Users/licairong/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/Users/licairong/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/Users/licairong/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/Users/licairong/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/Users/licairong/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# minhash\n",
    "minhashResult = doc_shingleId.flatMap(mapper_minhash)\n",
    "minhashResultList = minhashResult.take(101)\n",
    "\n",
    "sigMatrix = [0]*101\n",
    "for i in range(101):\n",
    "    index = minhashResultList[i][0] - 1\n",
    "    sigMatrix[index] = minhashResultList[i][1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc6423c",
   "metadata": {},
   "source": [
    "### Step3 : LSH\n",
    "用mapper_LSH對minhashResult = [(0, [hash1, ..., hash100]),...] 計算會hash到哪一個bucket  \n",
    "產生LSHresult，其中每一組key-value pair為((bandID, hash_value), [doc1, doc2, ...])  \n",
    "以紀錄哪一些doc會hash到一樣的值  \n",
    "並進一步用mapper_filter_candidate算出hash到同一個value的candidate list  \n",
    "key-value pair = (candidate list, 同一組candidate list出現的次數)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de607c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSH\n",
    "LSHresult = minhashResult.flatMap(mapper_LSH).reduceByKey(lambda x, y: x+y)\n",
    "LSHresult = LSHresult.flatMap(mapper_filter_candidate).reduceByKey(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad3b067",
   "metadata": {},
   "source": [
    "### Step4 : 算出最高similarity的documents\n",
    "用mapper_calSim算出所有candidate pair和他們的similarity,  \n",
    "因為重複的candidate pair可能出現，reduceByKey用來把重複的candidate pair刪掉  \n",
    "最終產生sim的key-value pair = ((docID1, docID2), similarity)  \n",
    "用mapper_switch將key-value pair轉成(similarity, (docID1, docID2))後，由大到小排序  \n",
    "並且印出前10大的similarity及document name，即為最終結果  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e86cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer_sim(x, y):\n",
    "    if x > y :\n",
    "        return x\n",
    "    else:\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "697a522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Jaccard similarity & write to file\n",
    "sim = LSHresult.flatMap(mapper_calSim).reduceByKey(reducer_sim)\n",
    "sim = sim.map(mapper_switch).sortByKey(False)\n",
    "\n",
    "fp = open(\"Outputfile.txt\",\"w\")\n",
    "similarity = sim.collect()\n",
    "for i in range(10):\n",
    "    if similarity[i][1][0]+1 > 0 and similarity[i][1][0]+1 <= 9: \n",
    "        doc1 = \"00\" + str(similarity[i][1][0]+1)\n",
    "    elif similarity[i][1][0]+1 > 9 and similarity[i][1][0]+1 <= 99: \n",
    "        doc1 = \"0\" + str(similarity[i][1][0]+1)\n",
    "    else: \n",
    "        doc1 = str(similarity[i][1][0]+1)\n",
    "        \n",
    "    if similarity[i][1][1]+1 > 0 and similarity[i][1][1]+1 <= 9: \n",
    "        doc2 = \"00\" + str(similarity[i][1][1]+1)\n",
    "    elif similarity[i][1][1]+1 > 9 and similarity[i][1][1]+1 <= 99: \n",
    "        doc2 = \"0\" + str(similarity[i][1][1]+1)\n",
    "    else: \n",
    "        doc2 = str(similarity[i][1][1]+1)\n",
    "            \n",
    "    num = round(similarity[i][0]*100, 2)\n",
    "    \n",
    "    fp.write(\"(\" + doc1 + \", \" + doc2 + \"): \" + \"{:.2f}\".format(num) + \"%\\n\")\n",
    "\n",
    "fp.seek(0)\n",
    "fp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "101eec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b257103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c98e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
